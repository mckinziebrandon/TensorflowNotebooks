{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Trying with TFLearn\n",
    "\n",
    "## It Works! Here's How. \n",
    "\n",
    "\n",
    "The following is a code snippet directly from [trainer.py](https://github.com/tflearn/tflearn/blob/master/tflearn/helpers/trainer.py#L281) in the tflearn github repository, where I'm only showing the relevant parts/logic. \n",
    "\n",
    "```python\n",
    "try:\n",
    "    for epoch in range(n_epoch):\n",
    "        # . . . Setup stuff for epoch here . . . \n",
    "        for batch_step in range(max_batches_len):\n",
    "            # . . . Setup stuff for next batch here . . . \n",
    "            for i, train_op in enumerate(self.train_ops):\n",
    "                caller.on_sub_batch_begin(self.training_state)\n",
    "                \n",
    "                # Train our model and store desired information in the train_op that\n",
    "                # we (the user) pass to the trainer as an initialization argument.\n",
    "                snapshot = train_op._train(self.training_state.step,\n",
    "                                           (bool(self.best_checkpoint_path) | snapshot_epoch),\n",
    "                                           snapshot_step,\n",
    "                                           show_metric)\n",
    "                                           \n",
    "                # Update training state. The training state object tells us \n",
    "                # how our model is doing at various stages of training.\n",
    "                self.training_state.update(train_op, train_ops_count)\n",
    "\n",
    "            # All optimizers batch end\n",
    "            self.session.run(self.incr_global_step)\n",
    "            caller.on_batch_end(self.training_state, snapshot)\n",
    "\n",
    "        # ---------- [What we care about] -------------\n",
    "        # Epoch end. We define what on_epoch_end does. In this\n",
    "        # case, I'll have it raise an exception if our validation accuracy\n",
    "        # reaches some desired threshold. \n",
    "        caller.on_epoch_end(self.training_state)\n",
    "        # ---------------------------------------------\n",
    "\n",
    "finally:\n",
    "    # Once we raise the exception, this code block will execute. \n",
    "    # Note only afterward will our catch block execute. \n",
    "    caller.on_train_end(self.training_state)\n",
    "    for t in self.train_ops:\n",
    "        t.train_dflow.interrupt()\n",
    "    # Set back train_ops\n",
    "    self.train_ops = original_train_ops\n",
    "```\n",
    "\n",
    "\n",
    "## Setup the Basic Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 not supported (please install/reinstall h5py)\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "trainX, trainY, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "n_features = 784\n",
    "n_hidden = 256\n",
    "n_classes = 10\n",
    "\n",
    "# Define the inputs/outputs/weights as usual.\n",
    "X = tf.placeholder(\"float\", [None, n_features])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define the connections/weights and biases between layers.\n",
    "W1 = tf.Variable(tf.random_normal([n_features, n_hidden]), name='W1')\n",
    "W2 = tf.Variable(tf.random_normal([n_hidden, n_hidden]), name='W2')\n",
    "W3 = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='W3')\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([n_hidden]), name='b1')\n",
    "b2 = tf.Variable(tf.random_normal([n_hidden]), name='b2')\n",
    "b3 = tf.Variable(tf.random_normal([n_classes]), name='b3')\n",
    "\n",
    "# Define the operations throughout the network.\n",
    "net = tf.tanh(tf.add(tf.matmul(X, W1), b1))\n",
    "net = tf.tanh(tf.add(tf.matmul(net, W2), b2))\n",
    "net = tf.add(tf.matmul(net, W3), b3)\n",
    "\n",
    "\n",
    "# Define the optimization problem.\n",
    "loss      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "accuracy  = tf.reduce_mean(tf.cast(\n",
    "        tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1) ), tf.float32), name='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the TrainOp and Trainer Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer, metric=accuracy, batch_size=128)\n",
    "trainer = tflearn.Trainer(train_ops=trainop, tensorboard_verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The EarlyStoppingCallback Class\n",
    "\n",
    "I show a proof-of-concept version of early stopping below. This is the simplest possible case: just stop training after the first epoch no matter what. It is up to the user to decide the conditions they want to trigger the stopping on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tflearn.callbacks.Callback):\n",
    "    def __init__(self, val_acc_thresh):\n",
    "        \"\"\" Note: We are free to define our init function however we please. \"\"\"\n",
    "        # Store a validation accuracy threshold, which we can compare against\n",
    "        # the current validation accuracy at, say, each epoch, each batch step, etc.\n",
    "        self.val_acc_thresh = val_acc_thresh\n",
    "    \n",
    "    def on_epoch_end(self, training_state):\n",
    "        \"\"\" \n",
    "        This is the final method called in trainer.py in the epoch loop. \n",
    "        We can stop training and leave without losing any information with a simple exception.  \n",
    "        \"\"\"\n",
    "        print(\"Terminating training at the end of epoch\", training_state.epoch)\n",
    "        raise StopIteration\n",
    "    \n",
    "    def on_train_end(self, training_state):\n",
    "        \"\"\"\n",
    "        Furthermore, tflearn will then immediately call this method after we terminate training, \n",
    "        (or when training ends regardless). This would be a good time to store any additional \n",
    "        information that tflearn doesn't store already.\n",
    "        \"\"\"\n",
    "        print(\"Successfully left training! Final model accuracy:\", training_state.acc_value)\n",
    "       \n",
    "        \n",
    "# Initialize our callback with desired accuracy threshold.  \n",
    "early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result: Train the Model and Stop Early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m1.73372\u001b[0m\u001b[0m\n",
      "| Optimizer | epoch: 002 | loss: 1.73372 - acc: 0.8196 | val_loss: 1.87058 - val_acc: 0.8011 -- iter: 55000/55000\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m1.73372\u001b[0m\u001b[0m\n",
      "| Optimizer | epoch: 002 | loss: 1.73372 - acc: 0.8196 | val_loss: 1.87058 - val_acc: 0.8011 -- iter: 55000/55000\n",
      "--\n",
      "Terminating training at the end of epoch 2\n",
      "Successfully left training! Final model accuracy: 0.8196054697036743\n",
      "Caught callback exception. Returning control to user program.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Give it to our trainer and let it fit the data. \n",
    "    trainer.fit(feed_dicts={X: trainX, Y: trainY}, \n",
    "                val_feed_dicts={X: testX, Y: testY}, \n",
    "                n_epoch=1, \n",
    "                show_metric=True, # Calculate accuracy and display at every step.\n",
    "                callbacks=early_stopping_cb)\n",
    "except StopIteration:\n",
    "    print(\"Caught callback exception. Returning control to user program.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "For my own reference, this is the code I started with before tinkering with the early stopping solution above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import urllib\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.io import arff\n",
    "\n",
    "import tflearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization, batch_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "\n",
    "# Load the data and handle any preprocessing here.\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "X, Y  = shuffle(X, Y)\n",
    "X     = X.reshape([-1, 28, 28, 1])\n",
    "testX = testX.reshape([-1, 28, 28, 1])\n",
    "\n",
    "# Define our network architecture: a simple 2-layer network of the form\n",
    "# InputImages -> Fully Connected -> Softmax\n",
    "out_readin1          = input_data(shape=[None,28,28,1])\n",
    "out_fully_connected2 = fully_connected(out_readin1, 10)\n",
    "out_softmax3         = fully_connected(out_fully_connected2, 10, activation='softmax')\n",
    "\n",
    "hash='f0c188c3777519fb93f1a825ca758a0c'\n",
    "scriptid='MNIST-f0c188c3777519fb93f1a825ca758a0c'\n",
    "\n",
    "# Define our training metrics. \n",
    "network = regression(out_softmax3, \n",
    "                     optimizer='adam', \n",
    "                     learning_rate=0.01, \n",
    "                     loss='categorical_crossentropy', \n",
    "                     name='target')\n",
    "\n",
    "model = tflearn.DNN(network, tensorboard_verbose=3)\n",
    "model.fit(X, Y, \n",
    "          n_epoch=1, \n",
    "          validation_set=(testX, testY), \n",
    "          snapshot_step=10, \n",
    "          snapshot_epoch=False, \n",
    "          show_metric=True, \n",
    "          run_id=scriptid)\n",
    "\n",
    "\n",
    "prediction = model.predict(testX)\n",
    "auc=roc_auc_score(testY, prediction, average='macro', sample_weight=None)\n",
    "accuracy=model.evaluate(testX,testY)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"ROC AUC Score:\", auc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
